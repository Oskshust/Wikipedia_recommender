{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Oskar Szudzik 148245\n",
    "* Krystian Moras 148243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step was to download articles from Wikipedia and save them in .csv file. You can find more code of the preprocessing in pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to load this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>links</th>\n",
       "      <th>categories</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Psilocybe yungensis</td>\n",
       "      <td>psilocybe yungensis is a species of psychedeli...</td>\n",
       "      <td>Agaricales|Agaricomycetes|Alexander H. Smith|A...</td>\n",
       "      <td>Articles with 'species' microformats|Articles ...</td>\n",
       "      <td>http://www.fungimag.com/summer-2011-articles/F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mian Rud, South Khorasan</td>\n",
       "      <td>mian rud (persian: ميان رود, also romanized as...</td>\n",
       "      <td>Abbasabad, Doreh|Abbasabad, Momenabad|Administ...</td>\n",
       "      <td>All stub articles|Articles containing Persian-...</td>\n",
       "      <td>http://geonames.nga.mil/namesgaz/|http://geoha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coat of arms of Hesse</td>\n",
       "      <td>the coat of arms of the  german state of hesse...</td>\n",
       "      <td>Armiger|Blazon|Coat of arms|Coat of arms of An...</td>\n",
       "      <td>All stub articles|Articles with short descript...</td>\n",
       "      <td>https://www.hessen.de/fuer-besucher/70-jahre-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Helsinki Police Department</td>\n",
       "      <td>the helsinki police department (hpd) (finnish:...</td>\n",
       "      <td>2018 Russia–United States summit|8th World Fes...</td>\n",
       "      <td>Government agencies established in 1826|Law en...</td>\n",
       "      <td>http://tass.com/politics/1013215/amp|https://b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Broadway Stages</td>\n",
       "      <td>broadway stages, ltd. is one of new york’s ful...</td>\n",
       "      <td>Annadale, Staten Island|Arden Heights, Staten ...</td>\n",
       "      <td>1983 establishments in New York City|American ...</td>\n",
       "      <td>http://Broadway-Stages.com/|http://www.broadwa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        title  \\\n",
       "0         Psilocybe yungensis   \n",
       "1    Mian Rud, South Khorasan   \n",
       "2       Coat of arms of Hesse   \n",
       "3  Helsinki Police Department   \n",
       "4             Broadway Stages   \n",
       "\n",
       "                                             summary  \\\n",
       "0  psilocybe yungensis is a species of psychedeli...   \n",
       "1  mian rud (persian: ميان رود, also romanized as...   \n",
       "2  the coat of arms of the  german state of hesse...   \n",
       "3  the helsinki police department (hpd) (finnish:...   \n",
       "4  broadway stages, ltd. is one of new york’s ful...   \n",
       "\n",
       "                                               links  \\\n",
       "0  Agaricales|Agaricomycetes|Alexander H. Smith|A...   \n",
       "1  Abbasabad, Doreh|Abbasabad, Momenabad|Administ...   \n",
       "2  Armiger|Blazon|Coat of arms|Coat of arms of An...   \n",
       "3  2018 Russia–United States summit|8th World Fes...   \n",
       "4  Annadale, Staten Island|Arden Heights, Staten ...   \n",
       "\n",
       "                                          categories  \\\n",
       "0  Articles with 'species' microformats|Articles ...   \n",
       "1  All stub articles|Articles containing Persian-...   \n",
       "2  All stub articles|Articles with short descript...   \n",
       "3  Government agencies established in 1826|Law en...   \n",
       "4  1983 establishments in New York City|American ...   \n",
       "\n",
       "                                          references  \n",
       "0  http://www.fungimag.com/summer-2011-articles/F...  \n",
       "1  http://geonames.nga.mil/namesgaz/|http://geoha...  \n",
       "2  https://www.hessen.de/fuer-besucher/70-jahre-h...  \n",
       "3  http://tass.com/politics/1013215/amp|https://b...  \n",
       "4  http://Broadway-Stages.com/|http://www.broadwa...  "
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('wikipedia.csv', header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform TF-IDF for summary of articles. To later propose similar articles to ones liked by a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'psilocybe yungensis is a species of psychedelic mushroom in the family hymenogastraceae. in north america, it is found in northeast, central and southeastern mexico. in south america, it has been recorded from bolivia, colombia, and ecuador. it is also known from the caribbean island martinique, and china. the mushroom grows in clusters or groups on rotting wood. the fruit bodies have conical to bell-shaped reddish- to orangish-brown caps that are up to 2.5 cm (1.0 in) in diameter, set atop slender stems 3 to 5 cm (1.2 to 2.0 in) long. the mushrooms stain blue when bruised, indicative of the presence of the compound psilocybin. psilocybe yungensis is used by mazatec indians in the mexican state of oaxaca for entheogenic purposes.'"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = data.iloc[0]\n",
    "article.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to tokenize the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "from math import log \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_words = {}\n",
    "\n",
    "for i in range(50):\n",
    "    article = data.iloc[i]\n",
    "    words = []\n",
    "\n",
    "    for sentence in sent_tokenize(article.summary):\n",
    "        words += word_tokenize(sentence)\n",
    "    dict_of_words[article.title] = words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compose normalized TF dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted({x for v in dict_of_words.values() for x in v})\n",
    "\n",
    "# all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict = {}\n",
    "for document in dict_of_words.keys():\n",
    "    counted = Counter(dict_of_words[document])\n",
    "    word_freq = {}\n",
    "    max_occurances = max(counted.values()) \n",
    "\n",
    "    for key, value in zip(counted.keys(), counted.values()):\n",
    "        word_freq[key] = value / max_occurances\n",
    "\n",
    "    for word in all_words:\n",
    "        if word in word_freq.keys():\n",
    "            continue\n",
    "        else:\n",
    "            word_freq[word] = 0\n",
    "\n",
    "    tf_dict[document] = word_freq\n",
    "\n",
    "# tf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And IDF as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dict = {}\n",
    "num_of_docs = len(dict_of_words.keys())\n",
    "for word in all_words:\n",
    "    word_occurances = 0\n",
    "    for article in dict_of_words.keys():\n",
    "        if word in dict_of_words[article]:\n",
    "            word_occurances += 1        \n",
    "    try:\n",
    "        idf_dict[word] = log(num_of_docs / word_occurances)\n",
    "    except:\n",
    "        # quotes, dates, some of the thrash\n",
    "        continue\n",
    "\n",
    "# idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to perform weighting for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = deepcopy(tf_dict)\n",
    "\n",
    "for document in tf_dict.keys():\n",
    "    for word in tf_dict[document].keys():\n",
    "        weight_dict[document][word] = tf_dict[document][word] * idf_dict[word]\n",
    "\n",
    "# weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we check cosine similarity between articles we have, and the one we liked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lois Delander',\n",
       " 'Broadway Stages',\n",
       " 'Premier League Riders Championship',\n",
       " '2011–12 TFF Second League',\n",
       " 'Soul Food (soundtrack)']"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liked_articles = ['Helsinki Police Department', 'William Stephen Devery']\n",
    "liked_weights = {}\n",
    "liked_w_abs = {}\n",
    "for l_article in liked_articles:\n",
    "    liked_weights[l_article] = weight_dict.pop(l_article)\n",
    "    liked_w_abs[l_article] = abs(sum(liked_weights[l_article].values()))\n",
    "\n",
    "cos_sim = {}\n",
    "for l_article in liked_articles:\n",
    "    for document in weight_dict.keys():\n",
    "        if document not in cos_sim.keys():\n",
    "            cos_sim[document] = 0 \n",
    "        for word in weight_dict[document].keys():\n",
    "            cos_sim[document] += liked_weights[l_article][word] * weight_dict[document][word]            \n",
    "        cos_sim[document] /= (liked_w_abs[l_article] * abs(sum(weight_dict[document].values())))\n",
    "        \n",
    "suggestions = dict(sorted(cos_sim.items(), key=lambda item: -item[1]))\n",
    "list(suggestions.keys())[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('tensor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8267083643c566585343ab60a0d1bac4aa9b9691d5fca7cd9cf8a09021580d4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
